{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessari**"
      ],
      "metadata": {
        "id": "MTN1jJ7eZlUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input\n",
        "from keras import models\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Y4t4b0qVFw4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Funzioni utilizzate**"
      ],
      "metadata": {
        "id": "YJnZ9fzXZrx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polar_generator(batchsize,grid=(10,10),noise=.002,flat=False):\n",
        "  while True:\n",
        "    x = np.random.rand(batchsize)\n",
        "    y = np.random.rand(batchsize)\n",
        "    out = np.zeros((batchsize,grid[0],grid[1]))\n",
        "    xc = (x*grid[0]).astype(int)\n",
        "    yc = (y*grid[1]).astype(int)\n",
        "    for b in range(batchsize):\n",
        "      out[b,xc[b],yc[b]] = 1\n",
        "    #compute rho and theta and add some noise\n",
        "    rho = np.sqrt(x**2+y**2) + np.random.normal(scale=noise)\n",
        "    theta = np.arctan(y/np.maximum(x,.00001)) + np.random.normal(scale=noise)\n",
        "    if flat:\n",
        "      out = np.reshape(out,(batchsize,grid[0]*grid[1]))\n",
        "    yield ((theta,rho),out)\n",
        "\n",
        "def my_accuracy_wrapper(real,predict):\n",
        "  # accuracy = true positive / (true positive + true negative)\n",
        "  np_r = real.numpy()\n",
        "  np_p = predict.numpy()\n",
        "  # cerco il massimo in entrambi ignorando quindi gli altri numeri \n",
        "  # i quali dovrebbe essere tutti zero per \"real\"\n",
        "  max_r = np.argmax(np_r, axis=1)\n",
        "  max_p = np.argmax(np_p, axis=1)\n",
        "  max_e = np.equal(max_r,max_p)\n",
        "  # questo somma solo gli 1 ovvero il numero dei positivi\n",
        "  true_positive = np.sum(max_e)\n",
        "  true_positive_true_negative = len(max_e)\n",
        "  return true_positive / true_positive_true_negative\n",
        "\n",
        "def my_accuracy(real, predict):\n",
        "  # Codice sorgente della categorical_accuracy di keras: \n",
        "  # K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))\n",
        "  # Converto i tensori in formati che numpy e' in grado di elaborare\n",
        "  return tf.py_function(\n",
        "      func=my_accuracy_wrapper,\n",
        "      inp=[real, predict],\n",
        "      Tout=tf.float32\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "cJJUP0wpFzFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definizione del modello**"
      ],
      "metadata": {
        "id": "VZDQMaVua-rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# la scelta di usare due input separati e' dovuta al ridotto numero di input che \n",
        "# abbiamo a dispozione e anche perche' i due input non sappiamo se hanno o meno\n",
        "# una relazione tra loro quindi decidiamo di considerarli almeno nella prima\n",
        "# fase in maniera indipendente\n",
        "theta = Input(shape=(1))\n",
        "rho = Input(shape=(1))\n",
        "# softsign e tanh sono state scelta in maniera sperimentale\n",
        "# provando per ordine: \n",
        "# (swish, relu), (tanh, relu), (swish, tanh), (sofsign, swish), (softsign, tanh)\n",
        "dense_1 = Dense(2, activation='softsign')(theta)\n",
        "dense_2 = Dense(2, activation='tanh')(rho)\n",
        "# inception module\n",
        "mid_1 = tf.keras.layers.concatenate([dense_1, dense_2], axis = 1)\n",
        "\n",
        "# elu sembra essere una versione migliorata della relu\n",
        "# la relu risulta una buona scelta nelle reti profonde\n",
        "elu_1= Dense(4, activation='elu')(mid_1)\n",
        "elu_2 = Dense(4, activation='elu')(elu_1)\n",
        "# la softmax permette di distribuire la probabilita'\n",
        "output = Dense(100, activation='softmax')(elu_2)\n",
        "\n",
        "model = Model([theta,rho], output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "KaLZEeqSa9_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fase di training del modello**"
      ],
      "metadata": {
        "id": "PvLWv9_kZuZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY2YU74oFtYN",
        "outputId": "8570c5c3-50d0-42aa-c3c9-a8dfa6d71a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 2)            4           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            4           ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4)            0           ['dense[0][0]',                  \n",
            "                                                                  'dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 4)            20          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 4)            20          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100)          500         ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 548\n",
            "Trainable params: 548\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/150\n",
            "1465/1465 [==============================] - 25s 15ms/step - loss: 3.0238 - accuracy: 0.2700 - my_accuracy: 0.2700\n",
            "Epoch 2/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 1.2140 - accuracy: 0.7754 - my_accuracy: 0.7755\n",
            "Epoch 3/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.6750 - accuracy: 0.8695 - my_accuracy: 0.8695\n",
            "Epoch 4/150\n",
            "1465/1465 [==============================] - 23s 16ms/step - loss: 0.4710 - accuracy: 0.9001 - my_accuracy: 0.9001\n",
            "Epoch 5/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.3720 - accuracy: 0.9169 - my_accuracy: 0.9169\n",
            "Epoch 6/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.3140 - accuracy: 0.9274 - my_accuracy: 0.9274\n",
            "Epoch 7/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.2762 - accuracy: 0.9348 - my_accuracy: 0.9348\n",
            "Epoch 8/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.2495 - accuracy: 0.9404 - my_accuracy: 0.9404\n",
            "Epoch 9/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.2291 - accuracy: 0.9451 - my_accuracy: 0.9451\n",
            "Epoch 10/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.2131 - accuracy: 0.9487 - my_accuracy: 0.9487\n",
            "Epoch 11/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.2002 - accuracy: 0.9516 - my_accuracy: 0.9516\n",
            "Epoch 12/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.1896 - accuracy: 0.9539 - my_accuracy: 0.9539\n",
            "Epoch 13/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1804 - accuracy: 0.9560 - my_accuracy: 0.9560\n",
            "Epoch 14/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1725 - accuracy: 0.9580 - my_accuracy: 0.9580\n",
            "Epoch 15/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.1657 - accuracy: 0.9592 - my_accuracy: 0.9592\n",
            "Epoch 16/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1597 - accuracy: 0.9607 - my_accuracy: 0.9607\n",
            "Epoch 17/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1544 - accuracy: 0.9616 - my_accuracy: 0.9616\n",
            "Epoch 18/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1497 - accuracy: 0.9625 - my_accuracy: 0.9625\n",
            "Epoch 19/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1452 - accuracy: 0.9637 - my_accuracy: 0.9637\n",
            "Epoch 20/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1413 - accuracy: 0.9643 - my_accuracy: 0.9643\n",
            "Epoch 21/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1375 - accuracy: 0.9653 - my_accuracy: 0.9653\n",
            "Epoch 22/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1343 - accuracy: 0.9659 - my_accuracy: 0.9659\n",
            "Epoch 23/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.1312 - accuracy: 0.9664 - my_accuracy: 0.9664\n",
            "Epoch 24/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1282 - accuracy: 0.9672 - my_accuracy: 0.9672\n",
            "Epoch 25/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1256 - accuracy: 0.9675 - my_accuracy: 0.9675\n",
            "Epoch 26/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1231 - accuracy: 0.9681 - my_accuracy: 0.9681\n",
            "Epoch 27/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.1206 - accuracy: 0.9686 - my_accuracy: 0.9686\n",
            "Epoch 28/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1184 - accuracy: 0.9691 - my_accuracy: 0.9691\n",
            "Epoch 29/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.1163 - accuracy: 0.9695 - my_accuracy: 0.9695\n",
            "Epoch 30/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1142 - accuracy: 0.9701 - my_accuracy: 0.9701\n",
            "Epoch 31/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1121 - accuracy: 0.9705 - my_accuracy: 0.9705\n",
            "Epoch 32/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1104 - accuracy: 0.9708 - my_accuracy: 0.9708\n",
            "Epoch 33/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1086 - accuracy: 0.9714 - my_accuracy: 0.9714\n",
            "Epoch 34/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1071 - accuracy: 0.9715 - my_accuracy: 0.9715\n",
            "Epoch 35/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1054 - accuracy: 0.9720 - my_accuracy: 0.9720\n",
            "Epoch 36/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1039 - accuracy: 0.9723 - my_accuracy: 0.9723\n",
            "Epoch 37/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.1027 - accuracy: 0.9722 - my_accuracy: 0.9722\n",
            "Epoch 38/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.1012 - accuracy: 0.9727 - my_accuracy: 0.9727\n",
            "Epoch 39/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.0999 - accuracy: 0.9730 - my_accuracy: 0.9730\n",
            "Epoch 40/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0987 - accuracy: 0.9731 - my_accuracy: 0.9731\n",
            "Epoch 41/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0975 - accuracy: 0.9734 - my_accuracy: 0.9734\n",
            "Epoch 42/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.0964 - accuracy: 0.9736 - my_accuracy: 0.9736\n",
            "Epoch 43/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.0952 - accuracy: 0.9740 - my_accuracy: 0.9740\n",
            "Epoch 44/150\n",
            "1465/1465 [==============================] - 19s 13ms/step - loss: 0.0942 - accuracy: 0.9741 - my_accuracy: 0.9741\n",
            "Epoch 45/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.0931 - accuracy: 0.9745 - my_accuracy: 0.9745\n",
            "Epoch 46/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0922 - accuracy: 0.9745 - my_accuracy: 0.9745\n",
            "Epoch 47/150\n",
            "1465/1465 [==============================] - 20s 13ms/step - loss: 0.0912 - accuracy: 0.9747 - my_accuracy: 0.9747\n",
            "Epoch 48/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0902 - accuracy: 0.9751 - my_accuracy: 0.9751\n",
            "Epoch 49/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0893 - accuracy: 0.9753 - my_accuracy: 0.9753\n",
            "Epoch 50/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0885 - accuracy: 0.9754 - my_accuracy: 0.9754\n",
            "Epoch 51/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0879 - accuracy: 0.9753 - my_accuracy: 0.9753\n",
            "Epoch 52/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0871 - accuracy: 0.9756 - my_accuracy: 0.9756\n",
            "Epoch 53/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0862 - accuracy: 0.9758 - my_accuracy: 0.9758\n",
            "Epoch 54/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0855 - accuracy: 0.9760 - my_accuracy: 0.9760\n",
            "Epoch 55/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0848 - accuracy: 0.9760 - my_accuracy: 0.9760\n",
            "Epoch 56/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0842 - accuracy: 0.9761 - my_accuracy: 0.9761\n",
            "Epoch 57/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0835 - accuracy: 0.9762 - my_accuracy: 0.9762\n",
            "Epoch 58/150\n",
            "1465/1465 [==============================] - 20s 14ms/step - loss: 0.0829 - accuracy: 0.9763 - my_accuracy: 0.9763\n",
            "Epoch 59/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0822 - accuracy: 0.9765 - my_accuracy: 0.9765\n",
            "Epoch 60/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0815 - accuracy: 0.9767 - my_accuracy: 0.9767\n",
            "Epoch 61/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0809 - accuracy: 0.9769 - my_accuracy: 0.9769\n",
            "Epoch 62/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0803 - accuracy: 0.9771 - my_accuracy: 0.9771\n",
            "Epoch 63/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0797 - accuracy: 0.9772 - my_accuracy: 0.9772\n",
            "Epoch 64/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0793 - accuracy: 0.9770 - my_accuracy: 0.9770\n",
            "Epoch 65/150\n",
            "1465/1465 [==============================] - 23s 16ms/step - loss: 0.0787 - accuracy: 0.9772 - my_accuracy: 0.9772\n",
            "Epoch 66/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0782 - accuracy: 0.9774 - my_accuracy: 0.9774\n",
            "Epoch 67/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0777 - accuracy: 0.9774 - my_accuracy: 0.9774\n",
            "Epoch 68/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0774 - accuracy: 0.9773 - my_accuracy: 0.9773\n",
            "Epoch 69/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0767 - accuracy: 0.9776 - my_accuracy: 0.9776\n",
            "Epoch 70/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0763 - accuracy: 0.9777 - my_accuracy: 0.9777\n",
            "Epoch 71/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0758 - accuracy: 0.9777 - my_accuracy: 0.9777\n",
            "Epoch 72/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0753 - accuracy: 0.9779 - my_accuracy: 0.9779\n",
            "Epoch 73/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0749 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 74/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0744 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 75/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0740 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 76/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0735 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 77/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0731 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 78/150\n",
            "1465/1465 [==============================] - 23s 15ms/step - loss: 0.0727 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 79/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0725 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 80/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0721 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 81/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0717 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 82/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0713 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 83/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0709 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 84/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0707 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 85/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0703 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 86/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0699 - accuracy: 0.9789 - my_accuracy: 0.9789\n",
            "Epoch 87/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0696 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 88/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0694 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 89/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0690 - accuracy: 0.9790 - my_accuracy: 0.9790\n",
            "Epoch 90/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0687 - accuracy: 0.9789 - my_accuracy: 0.9789\n",
            "Epoch 91/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0683 - accuracy: 0.9791 - my_accuracy: 0.9791\n",
            "Epoch 92/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0680 - accuracy: 0.9792 - my_accuracy: 0.9792\n",
            "Epoch 93/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0678 - accuracy: 0.9790 - my_accuracy: 0.9790\n",
            "Epoch 94/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0676 - accuracy: 0.9791 - my_accuracy: 0.9791\n",
            "Epoch 95/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0672 - accuracy: 0.9793 - my_accuracy: 0.9793\n",
            "Epoch 96/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0669 - accuracy: 0.9793 - my_accuracy: 0.9793\n",
            "Epoch 97/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0668 - accuracy: 0.9792 - my_accuracy: 0.9792\n",
            "Epoch 98/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0664 - accuracy: 0.9793 - my_accuracy: 0.9793\n",
            "Epoch 99/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0663 - accuracy: 0.9792 - my_accuracy: 0.9792\n",
            "Epoch 100/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0658 - accuracy: 0.9795 - my_accuracy: 0.9795\n",
            "Epoch 101/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0658 - accuracy: 0.9794 - my_accuracy: 0.9794\n",
            "Epoch 102/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0655 - accuracy: 0.9794 - my_accuracy: 0.9794\n",
            "Epoch 103/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0653 - accuracy: 0.9794 - my_accuracy: 0.9794\n",
            "Epoch 104/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0649 - accuracy: 0.9794 - my_accuracy: 0.9794\n",
            "Epoch 105/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0648 - accuracy: 0.9795 - my_accuracy: 0.9795\n",
            "Epoch 106/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0644 - accuracy: 0.9796 - my_accuracy: 0.9796\n",
            "Epoch 107/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0642 - accuracy: 0.9797 - my_accuracy: 0.9797\n",
            "Epoch 108/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0641 - accuracy: 0.9796 - my_accuracy: 0.9796\n",
            "Epoch 109/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0637 - accuracy: 0.9798 - my_accuracy: 0.9798\n",
            "Epoch 110/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0635 - accuracy: 0.9797 - my_accuracy: 0.9797\n",
            "Epoch 111/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0632 - accuracy: 0.9799 - my_accuracy: 0.9799\n",
            "Epoch 112/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0631 - accuracy: 0.9799 - my_accuracy: 0.9799\n",
            "Epoch 113/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0629 - accuracy: 0.9800 - my_accuracy: 0.9800\n",
            "Epoch 114/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0627 - accuracy: 0.9798 - my_accuracy: 0.9798\n",
            "Epoch 115/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0624 - accuracy: 0.9800 - my_accuracy: 0.9800\n",
            "Epoch 116/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0622 - accuracy: 0.9800 - my_accuracy: 0.9800\n",
            "Epoch 117/150\n",
            "1465/1465 [==============================] - 23s 15ms/step - loss: 0.0622 - accuracy: 0.9798 - my_accuracy: 0.9798\n",
            "Epoch 118/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0620 - accuracy: 0.9800 - my_accuracy: 0.9800\n",
            "Epoch 119/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0617 - accuracy: 0.9801 - my_accuracy: 0.9801\n",
            "Epoch 120/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0615 - accuracy: 0.9801 - my_accuracy: 0.9801\n",
            "Epoch 121/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0614 - accuracy: 0.9800 - my_accuracy: 0.9800\n",
            "Epoch 122/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0612 - accuracy: 0.9800 - my_accuracy: 0.9800\n",
            "Epoch 123/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0609 - accuracy: 0.9804 - my_accuracy: 0.9804\n",
            "Epoch 124/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0607 - accuracy: 0.9802 - my_accuracy: 0.9802\n",
            "Epoch 125/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0607 - accuracy: 0.9802 - my_accuracy: 0.9802\n",
            "Epoch 126/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0605 - accuracy: 0.9802 - my_accuracy: 0.9802\n",
            "Epoch 127/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0602 - accuracy: 0.9803 - my_accuracy: 0.9803\n",
            "Epoch 128/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0602 - accuracy: 0.9802 - my_accuracy: 0.9802\n",
            "Epoch 129/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0599 - accuracy: 0.9804 - my_accuracy: 0.9804\n",
            "Epoch 130/150\n",
            "1465/1465 [==============================] - 23s 16ms/step - loss: 0.0599 - accuracy: 0.9802 - my_accuracy: 0.9802\n",
            "Epoch 131/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0595 - accuracy: 0.9806 - my_accuracy: 0.9806\n",
            "Epoch 132/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0594 - accuracy: 0.9805 - my_accuracy: 0.9805\n",
            "Epoch 133/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0593 - accuracy: 0.9804 - my_accuracy: 0.9804\n",
            "Epoch 134/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0592 - accuracy: 0.9804 - my_accuracy: 0.9804\n",
            "Epoch 135/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0589 - accuracy: 0.9805 - my_accuracy: 0.9805\n",
            "Epoch 136/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0587 - accuracy: 0.9806 - my_accuracy: 0.9806\n",
            "Epoch 137/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0587 - accuracy: 0.9805 - my_accuracy: 0.9805\n",
            "Epoch 138/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0585 - accuracy: 0.9805 - my_accuracy: 0.9805\n",
            "Epoch 139/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0583 - accuracy: 0.9806 - my_accuracy: 0.9806\n",
            "Epoch 140/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0582 - accuracy: 0.9806 - my_accuracy: 0.9806\n",
            "Epoch 141/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0579 - accuracy: 0.9807 - my_accuracy: 0.9807\n",
            "Epoch 142/150\n",
            "1465/1465 [==============================] - 21s 14ms/step - loss: 0.0580 - accuracy: 0.9806 - my_accuracy: 0.9806\n",
            "Epoch 143/150\n",
            "1465/1465 [==============================] - 23s 15ms/step - loss: 0.0580 - accuracy: 0.9805 - my_accuracy: 0.9805\n",
            "Epoch 144/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0577 - accuracy: 0.9807 - my_accuracy: 0.9807\n",
            "Epoch 145/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0575 - accuracy: 0.9807 - my_accuracy: 0.9807\n",
            "Epoch 146/150\n",
            "1465/1465 [==============================] - 21s 15ms/step - loss: 0.0574 - accuracy: 0.9807 - my_accuracy: 0.9807\n",
            "Epoch 147/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0572 - accuracy: 0.9807 - my_accuracy: 0.9807\n",
            "Epoch 148/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0571 - accuracy: 0.9808 - my_accuracy: 0.9808\n",
            "Epoch 149/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0570 - accuracy: 0.9808 - my_accuracy: 0.9808\n",
            "Epoch 150/150\n",
            "1465/1465 [==============================] - 22s 15ms/step - loss: 0.0568 - accuracy: 0.9809 - my_accuracy: 0.9809\n"
          ]
        }
      ],
      "source": [
        "n_train = 3000000\n",
        "\n",
        "g1,g2 = 10,10\n",
        "gen = polar_generator(n_train,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "# epoche e batch size sono stati scelti abbastanza alti perche' nella prima fase\n",
        "# lo scopo e' quello di aumentare quanto possibile l'accuracy\n",
        "\n",
        "epochs = 150\n",
        "batch_size = 2048\n",
        "\n",
        "opt = keras.optimizers.Adam()\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', \n",
        "        metrics=['accuracy', my_accuracy]\n",
        "              )\n",
        "\n",
        "(theta,rho),y = next(gen)\n",
        "\n",
        "# alleno la rete la prima volta, senza validation set per velocizzare il processo\n",
        "history = model.fit((theta,rho), y, epochs=epochs, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nella seconda fase di train il numero di dati nel training set e' diminuito\n",
        "# le epoche sono state ridotte a 50 e il batch size 1024\n",
        "# in quanto bisogna iniziare a temere overfitting e migliorare invece la precisione\n",
        "\n",
        "n_train = 1000000\n",
        "\n",
        "g1,g2 = 10,10\n",
        "gen = polar_generator(n_train,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 1024\n",
        "\n",
        "opt = keras.optimizers.Adam()\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', \n",
        "        metrics=['accuracy', my_accuracy]\n",
        "              )\n",
        "\n",
        "(theta,rho),y = next(gen)\n",
        "\n",
        "history = model.fit((theta,rho), y, epochs=epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bzz1oTlNhlC",
        "outputId": "ce1c9319-6c35-445e-9067-d720e1da35fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "977/977 [==============================] - 10s 9ms/step - loss: 0.0607 - accuracy: 0.9775 - my_accuracy: 0.9775\n",
            "Epoch 2/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0600 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 3/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0603 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 4/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0600 - accuracy: 0.9779 - my_accuracy: 0.9779\n",
            "Epoch 5/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0604 - accuracy: 0.9776 - my_accuracy: 0.9776\n",
            "Epoch 6/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0604 - accuracy: 0.9775 - my_accuracy: 0.9775\n",
            "Epoch 7/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0605 - accuracy: 0.9776 - my_accuracy: 0.9776\n",
            "Epoch 8/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0597 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 9/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0605 - accuracy: 0.9776 - my_accuracy: 0.9776\n",
            "Epoch 10/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0602 - accuracy: 0.9776 - my_accuracy: 0.9776\n",
            "Epoch 11/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0599 - accuracy: 0.9779 - my_accuracy: 0.9779\n",
            "Epoch 12/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0597 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 13/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0599 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 14/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0600 - accuracy: 0.9777 - my_accuracy: 0.9777\n",
            "Epoch 15/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0597 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 16/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0598 - accuracy: 0.9777 - my_accuracy: 0.9777\n",
            "Epoch 17/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0598 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 18/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0597 - accuracy: 0.9777 - my_accuracy: 0.9777\n",
            "Epoch 19/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0597 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 20/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0593 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 21/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0595 - accuracy: 0.9779 - my_accuracy: 0.9779\n",
            "Epoch 22/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0597 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 23/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0590 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 24/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0591 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 25/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0593 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 26/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0593 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 27/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0592 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 28/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0591 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 29/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0590 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 30/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0591 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 31/50\n",
            "977/977 [==============================] - 14s 14ms/step - loss: 0.0589 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 32/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0584 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 33/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0592 - accuracy: 0.9777 - my_accuracy: 0.9777\n",
            "Epoch 34/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0588 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 35/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0588 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 36/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0586 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 37/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0584 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 38/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0587 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 39/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0585 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 40/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0581 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 41/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0587 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 42/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0586 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 43/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0587 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 44/50\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0585 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 45/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0583 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 46/50\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0580 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 47/50\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0584 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 48/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0584 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 49/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0581 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 50/50\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0582 - accuracy: 0.9781 - my_accuracy: 0.9781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# la rete viene nuovamente allenata come nella fase precedente ma per piu' tempo\n",
        "\n",
        "n_train = 1000000\n",
        "\n",
        "g1,g2 = 10,10\n",
        "gen = polar_generator(n_train,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 1024\n",
        "\n",
        "opt = keras.optimizers.Adam()\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', \n",
        "        metrics=['accuracy', my_accuracy]\n",
        "              )\n",
        "\n",
        "(theta,rho),y = next(gen)\n",
        "\n",
        "history = model.fit((theta,rho), y, epochs=epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGLKw_gDI_yn",
        "outputId": "ba1fa0f2-26d5-4715-a10a-330833193f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0585 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 2/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0588 - accuracy: 0.9776 - my_accuracy: 0.9776\n",
            "Epoch 3/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0584 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 4/100\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0583 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 5/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0576 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 6/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0588 - accuracy: 0.9777 - my_accuracy: 0.9777\n",
            "Epoch 7/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0581 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 8/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0580 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 9/100\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0582 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 10/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0582 - accuracy: 0.9779 - my_accuracy: 0.9779\n",
            "Epoch 11/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0577 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 12/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0579 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 13/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0578 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 14/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0576 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 15/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0575 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 16/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0579 - accuracy: 0.9779 - my_accuracy: 0.9779\n",
            "Epoch 17/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0574 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 18/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0578 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 19/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0575 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 20/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0576 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 21/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0574 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 22/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0577 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 23/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0577 - accuracy: 0.9780 - my_accuracy: 0.9780\n",
            "Epoch 24/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0570 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 25/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0574 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 26/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0572 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 27/100\n",
            "977/977 [==============================] - 13s 14ms/step - loss: 0.0572 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 28/100\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0570 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 29/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0572 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 30/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0570 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 31/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0572 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 32/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0571 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 33/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0570 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 34/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0569 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 35/100\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0573 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 36/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0566 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 37/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0567 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 38/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0572 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 39/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0569 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 40/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0571 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 41/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0567 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 42/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0569 - accuracy: 0.9782 - my_accuracy: 0.9782\n",
            "Epoch 43/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0564 - accuracy: 0.9785 - my_accuracy: 0.9786\n",
            "Epoch 44/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0570 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 45/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0564 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 46/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0567 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 47/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0564 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 48/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0565 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 49/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0567 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 50/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0564 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 51/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0563 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 52/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0562 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 53/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0563 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 54/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0565 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 55/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0565 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 56/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0561 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 57/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0563 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 58/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0559 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 59/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0563 - accuracy: 0.9783 - my_accuracy: 0.9783\n",
            "Epoch 60/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0559 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 61/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0562 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 62/100\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0562 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 63/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0559 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 64/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0561 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 65/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0561 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 66/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0557 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 67/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0561 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 68/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0558 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 69/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0556 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 70/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0557 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 71/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0557 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 72/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0558 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 73/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0560 - accuracy: 0.9784 - my_accuracy: 0.9784\n",
            "Epoch 74/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0557 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 75/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0557 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 76/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0555 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 77/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0553 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 78/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0555 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 79/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0557 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 80/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0556 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 81/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0554 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 82/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0553 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 83/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0551 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 84/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0551 - accuracy: 0.9789 - my_accuracy: 0.9789\n",
            "Epoch 85/100\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0552 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 86/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0553 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 87/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0550 - accuracy: 0.9789 - my_accuracy: 0.9789\n",
            "Epoch 88/100\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0556 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 89/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0548 - accuracy: 0.9790 - my_accuracy: 0.9790\n",
            "Epoch 90/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0554 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 91/100\n",
            "977/977 [==============================] - 14s 14ms/step - loss: 0.0553 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 92/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0551 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 93/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0549 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 94/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0547 - accuracy: 0.9790 - my_accuracy: 0.9790\n",
            "Epoch 95/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0552 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 96/100\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0553 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 97/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0550 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 98/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0549 - accuracy: 0.9788 - my_accuracy: 0.9788\n",
            "Epoch 99/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0553 - accuracy: 0.9786 - my_accuracy: 0.9786\n",
            "Epoch 100/100\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0545 - accuracy: 0.9790 - my_accuracy: 0.9790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# la rete attualmente e' gia abbastanza accurata, per evitare overfitting\n",
        "# ma migliorare comunque la precisione le epoche sono notevolmente ridotte\n",
        "# e il learning rate abbassato di molto\n",
        "\n",
        "n_train = 1000000\n",
        "\n",
        "g1,g2 = 10,10\n",
        "gen = polar_generator(n_train,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 1024\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0000001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', \n",
        "        metrics=['accuracy', my_accuracy]\n",
        "              )\n",
        "\n",
        "(theta,rho),y = next(gen)\n",
        "\n",
        "history = model.fit((theta,rho), y, epochs=epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nCfb8YcJbTY",
        "outputId": "f36f1ce2-c591-4178-c18c-c040d7748b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0649 - accuracy: 0.9719 - my_accuracy: 0.9719\n",
            "Epoch 2/20\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0635 - accuracy: 0.9727 - my_accuracy: 0.9727\n",
            "Epoch 3/20\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0623 - accuracy: 0.9734 - my_accuracy: 0.9734\n",
            "Epoch 4/20\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0611 - accuracy: 0.9741 - my_accuracy: 0.9741\n",
            "Epoch 5/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0601 - accuracy: 0.9747 - my_accuracy: 0.9747\n",
            "Epoch 6/20\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0591 - accuracy: 0.9752 - my_accuracy: 0.9752\n",
            "Epoch 7/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0583 - accuracy: 0.9757 - my_accuracy: 0.9757\n",
            "Epoch 8/20\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0575 - accuracy: 0.9762 - my_accuracy: 0.9762\n",
            "Epoch 9/20\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0567 - accuracy: 0.9767 - my_accuracy: 0.9767\n",
            "Epoch 10/20\n",
            "977/977 [==============================] - 9s 9ms/step - loss: 0.0561 - accuracy: 0.9771 - my_accuracy: 0.9771\n",
            "Epoch 11/20\n",
            "977/977 [==============================] - 11s 11ms/step - loss: 0.0555 - accuracy: 0.9775 - my_accuracy: 0.9775\n",
            "Epoch 12/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0549 - accuracy: 0.9778 - my_accuracy: 0.9778\n",
            "Epoch 13/20\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0544 - accuracy: 0.9781 - my_accuracy: 0.9781\n",
            "Epoch 14/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0539 - accuracy: 0.9785 - my_accuracy: 0.9785\n",
            "Epoch 15/20\n",
            "977/977 [==============================] - 9s 10ms/step - loss: 0.0535 - accuracy: 0.9787 - my_accuracy: 0.9787\n",
            "Epoch 16/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0531 - accuracy: 0.9790 - my_accuracy: 0.9790\n",
            "Epoch 17/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0528 - accuracy: 0.9793 - my_accuracy: 0.9793\n",
            "Epoch 18/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0524 - accuracy: 0.9796 - my_accuracy: 0.9796\n",
            "Epoch 19/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0521 - accuracy: 0.9798 - my_accuracy: 0.9798\n",
            "Epoch 20/20\n",
            "977/977 [==============================] - 10s 10ms/step - loss: 0.0518 - accuracy: 0.9800 - my_accuracy: 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fase di validazione del modello**"
      ],
      "metadata": {
        "id": "-orRA2fPX9su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# il modello viene validato su un validation set di 20000 elementi come\n",
        "# da consegna e un noise di 0.002\n",
        "# per verificare la qualita' delle predizioni viene effettuata l'operazione\n",
        "# di evaluate su un set di 20000 per 1000 volte, per ogni ciclo viene verificato\n",
        "# se l'accuracy risulta sotto il 95%\n",
        "# il risultato e' stato che con abbastanza frequenza su 1000 cicli di evaluate \n",
        "# con validation set da 20000 elementi la percentuale di tentativi con accuracy\n",
        "# inferiore a 95% e' minore del 10%\n",
        "\n",
        "# prima verifica con esito\n",
        "#  7.6 % sotto al 95 %\n",
        "# 92.4 % sopra al 95 %\n",
        "\n",
        "g1,g2 = 10,10\n",
        "batch_size = 256\n",
        "n_test = 20000\n",
        "gen = polar_generator(n_test,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "accs = []\n",
        "mm = 0\n",
        "tot = 1000\n",
        "for x in range(tot):\n",
        "  (theta,rho),y = next(gen)\n",
        "  score, _, acc = model.evaluate((theta,rho), y, batch_size=batch_size,verbose=0)\n",
        "  if acc < 0.95:\n",
        "    mm += 1\n",
        "    print(\"Sotto 95% : {}/{}\".format(mm, x+1))\n",
        "  accs.append(acc)\n",
        "\n",
        "\n",
        "acc = np.mean(accs)\n",
        "print('Accuracy: {:.1f}%'.format(acc*100))\n",
        "print('Totali sotto 95% : {}/{}'.format(mm,tot))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVced9AHKB0Y",
        "outputId": "d74519df-28ec-4bf5-eac6-847e3a36b729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sotto 95% : 1/10\n",
            "Sotto 95% : 2/23\n",
            "Sotto 95% : 3/27\n",
            "Sotto 95% : 4/32\n",
            "Sotto 95% : 5/63\n",
            "Sotto 95% : 6/70\n",
            "Sotto 95% : 7/77\n",
            "Sotto 95% : 8/93\n",
            "Sotto 95% : 9/97\n",
            "Sotto 95% : 10/151\n",
            "Sotto 95% : 11/161\n",
            "Sotto 95% : 12/191\n",
            "Sotto 95% : 13/196\n",
            "Sotto 95% : 14/216\n",
            "Sotto 95% : 15/221\n",
            "Sotto 95% : 16/222\n",
            "Sotto 95% : 17/233\n",
            "Sotto 95% : 18/240\n",
            "Sotto 95% : 19/254\n",
            "Sotto 95% : 20/256\n",
            "Sotto 95% : 21/263\n",
            "Sotto 95% : 22/274\n",
            "Sotto 95% : 23/311\n",
            "Sotto 95% : 24/317\n",
            "Sotto 95% : 25/320\n",
            "Sotto 95% : 26/321\n",
            "Sotto 95% : 27/342\n",
            "Sotto 95% : 28/354\n",
            "Sotto 95% : 29/380\n",
            "Sotto 95% : 30/389\n",
            "Sotto 95% : 31/395\n",
            "Sotto 95% : 32/398\n",
            "Sotto 95% : 33/406\n",
            "Sotto 95% : 34/418\n",
            "Sotto 95% : 35/440\n",
            "Sotto 95% : 36/486\n",
            "Sotto 95% : 37/487\n",
            "Sotto 95% : 38/490\n",
            "Sotto 95% : 39/492\n",
            "Sotto 95% : 40/499\n",
            "Sotto 95% : 41/507\n",
            "Sotto 95% : 42/520\n",
            "Sotto 95% : 43/528\n",
            "Sotto 95% : 44/549\n",
            "Sotto 95% : 45/558\n",
            "Sotto 95% : 46/563\n",
            "Sotto 95% : 47/568\n",
            "Sotto 95% : 48/575\n",
            "Sotto 95% : 49/586\n",
            "Sotto 95% : 50/587\n",
            "Sotto 95% : 51/603\n",
            "Sotto 95% : 52/656\n",
            "Sotto 95% : 53/657\n",
            "Sotto 95% : 54/665\n",
            "Sotto 95% : 55/689\n",
            "Sotto 95% : 56/700\n",
            "Sotto 95% : 57/701\n",
            "Sotto 95% : 58/725\n",
            "Sotto 95% : 59/737\n",
            "Sotto 95% : 60/751\n",
            "Sotto 95% : 61/767\n",
            "Sotto 95% : 62/805\n",
            "Sotto 95% : 63/809\n",
            "Sotto 95% : 64/817\n",
            "Sotto 95% : 65/835\n",
            "Sotto 95% : 66/853\n",
            "Sotto 95% : 67/879\n",
            "Sotto 95% : 68/885\n",
            "Sotto 95% : 69/900\n",
            "Sotto 95% : 70/961\n",
            "Sotto 95% : 71/971\n",
            "Sotto 95% : 72/973\n",
            "Sotto 95% : 73/977\n",
            "Sotto 95% : 74/978\n",
            "Sotto 95% : 75/995\n",
            "Sotto 95% : 76/997\n",
            "Accuracy: 97.0%\n",
            "Totali sotto 95% : 76/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seconda verifica con esito\n",
        "#  5.9 % sotto al 95 %\n",
        "# 94.1 % sopra al 95 %\n",
        "\n",
        "g1,g2 = 10,10\n",
        "batch_size = 256\n",
        "n_test = 20000\n",
        "gen = polar_generator(n_test,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "accs = []\n",
        "mm = 0\n",
        "tot = 1000\n",
        "for x in range(tot):\n",
        "  (theta,rho),y = next(gen)\n",
        "  score, _, acc = model.evaluate((theta,rho), y, batch_size=batch_size,verbose=0)\n",
        "  if acc < 0.95:\n",
        "    mm += 1\n",
        "    print(\"Sotto 95% : {}/{}\".format(mm, x+1))\n",
        "  accs.append(acc)\n",
        "\n",
        "\n",
        "acc = np.mean(accs)\n",
        "print('Accuracy: {:.1f}%'.format(acc*100))\n",
        "print('Totali sotto 95% : {}/{}'.format(mm,tot))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNqGYbvy00wA",
        "outputId": "77265227-d7e3-4698-d0e7-3ab8320e48e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sotto 95% : 1/1\n",
            "Sotto 95% : 2/37\n",
            "Sotto 95% : 3/80\n",
            "Sotto 95% : 4/99\n",
            "Sotto 95% : 5/117\n",
            "Sotto 95% : 6/173\n",
            "Sotto 95% : 7/177\n",
            "Sotto 95% : 8/196\n",
            "Sotto 95% : 9/221\n",
            "Sotto 95% : 10/223\n",
            "Sotto 95% : 11/226\n",
            "Sotto 95% : 12/229\n",
            "Sotto 95% : 13/243\n",
            "Sotto 95% : 14/274\n",
            "Sotto 95% : 15/290\n",
            "Sotto 95% : 16/297\n",
            "Sotto 95% : 17/304\n",
            "Sotto 95% : 18/308\n",
            "Sotto 95% : 19/313\n",
            "Sotto 95% : 20/314\n",
            "Sotto 95% : 21/321\n",
            "Sotto 95% : 22/339\n",
            "Sotto 95% : 23/352\n",
            "Sotto 95% : 24/362\n",
            "Sotto 95% : 25/381\n",
            "Sotto 95% : 26/389\n",
            "Sotto 95% : 27/408\n",
            "Sotto 95% : 28/411\n",
            "Sotto 95% : 29/416\n",
            "Sotto 95% : 30/444\n",
            "Sotto 95% : 31/468\n",
            "Sotto 95% : 32/487\n",
            "Sotto 95% : 33/493\n",
            "Sotto 95% : 34/522\n",
            "Sotto 95% : 35/585\n",
            "Sotto 95% : 36/611\n",
            "Sotto 95% : 37/612\n",
            "Sotto 95% : 38/637\n",
            "Sotto 95% : 39/669\n",
            "Sotto 95% : 40/691\n",
            "Sotto 95% : 41/702\n",
            "Sotto 95% : 42/724\n",
            "Sotto 95% : 43/731\n",
            "Sotto 95% : 44/740\n",
            "Sotto 95% : 45/748\n",
            "Sotto 95% : 46/774\n",
            "Sotto 95% : 47/785\n",
            "Sotto 95% : 48/822\n",
            "Sotto 95% : 49/831\n",
            "Sotto 95% : 50/835\n",
            "Sotto 95% : 51/844\n",
            "Sotto 95% : 52/848\n",
            "Sotto 95% : 53/861\n",
            "Sotto 95% : 54/902\n",
            "Sotto 95% : 55/906\n",
            "Sotto 95% : 56/913\n",
            "Sotto 95% : 57/947\n",
            "Sotto 95% : 58/957\n",
            "Sotto 95% : 59/988\n",
            "Accuracy: 97.0%\n",
            "Totali sotto 95% : 59/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# terza verifica con esito\n",
        "#  7.0 % sotto al 95 %\n",
        "# 93.0 % sopra al 95 %\n",
        "\n",
        "g1,g2 = 10,10\n",
        "batch_size = 256\n",
        "n_test = 20000\n",
        "gen = polar_generator(n_test,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "accs = []\n",
        "mm = 0\n",
        "tot = 1000\n",
        "for x in range(tot):\n",
        "  (theta,rho),y = next(gen)\n",
        "  score, _, acc = model.evaluate((theta,rho), y, batch_size=batch_size,verbose=0)\n",
        "  if acc < 0.95:\n",
        "    mm += 1\n",
        "    print(\"Sotto 95% : {}/{}\".format(mm, x+1))\n",
        "  accs.append(acc)\n",
        "\n",
        "\n",
        "acc = np.mean(accs)\n",
        "print('Accuracy: {:.1f}%'.format(acc*100))\n",
        "print('Totali sotto 95% : {}/{}'.format(mm,tot))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD5Ciqt321as",
        "outputId": "38341b9b-f2f5-416d-ac3e-6df33b7c8440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sotto 95% : 1/1\n",
            "Sotto 95% : 2/3\n",
            "Sotto 95% : 3/6\n",
            "Sotto 95% : 4/18\n",
            "Sotto 95% : 5/34\n",
            "Sotto 95% : 6/44\n",
            "Sotto 95% : 7/56\n",
            "Sotto 95% : 8/69\n",
            "Sotto 95% : 9/74\n",
            "Sotto 95% : 10/100\n",
            "Sotto 95% : 11/125\n",
            "Sotto 95% : 12/126\n",
            "Sotto 95% : 13/139\n",
            "Sotto 95% : 14/140\n",
            "Sotto 95% : 15/189\n",
            "Sotto 95% : 16/192\n",
            "Sotto 95% : 17/223\n",
            "Sotto 95% : 18/259\n",
            "Sotto 95% : 19/274\n",
            "Sotto 95% : 20/277\n",
            "Sotto 95% : 21/290\n",
            "Sotto 95% : 22/298\n",
            "Sotto 95% : 23/306\n",
            "Sotto 95% : 24/309\n",
            "Sotto 95% : 25/313\n",
            "Sotto 95% : 26/339\n",
            "Sotto 95% : 27/344\n",
            "Sotto 95% : 28/348\n",
            "Sotto 95% : 29/349\n",
            "Sotto 95% : 30/354\n",
            "Sotto 95% : 31/356\n",
            "Sotto 95% : 32/358\n",
            "Sotto 95% : 33/418\n",
            "Sotto 95% : 34/429\n",
            "Sotto 95% : 35/452\n",
            "Sotto 95% : 36/483\n",
            "Sotto 95% : 37/487\n",
            "Sotto 95% : 38/500\n",
            "Sotto 95% : 39/522\n",
            "Sotto 95% : 40/556\n",
            "Sotto 95% : 41/562\n",
            "Sotto 95% : 42/578\n",
            "Sotto 95% : 43/588\n",
            "Sotto 95% : 44/592\n",
            "Sotto 95% : 45/633\n",
            "Sotto 95% : 46/642\n",
            "Sotto 95% : 47/657\n",
            "Sotto 95% : 48/664\n",
            "Sotto 95% : 49/684\n",
            "Sotto 95% : 50/711\n",
            "Sotto 95% : 51/744\n",
            "Sotto 95% : 52/775\n",
            "Sotto 95% : 53/776\n",
            "Sotto 95% : 54/784\n",
            "Sotto 95% : 55/785\n",
            "Sotto 95% : 56/831\n",
            "Sotto 95% : 57/839\n",
            "Sotto 95% : 58/843\n",
            "Sotto 95% : 59/887\n",
            "Sotto 95% : 60/890\n",
            "Sotto 95% : 61/896\n",
            "Sotto 95% : 62/924\n",
            "Sotto 95% : 63/932\n",
            "Sotto 95% : 64/936\n",
            "Sotto 95% : 65/938\n",
            "Sotto 95% : 66/948\n",
            "Sotto 95% : 67/956\n",
            "Sotto 95% : 68/958\n",
            "Sotto 95% : 69/978\n",
            "Sotto 95% : 70/996\n",
            "Accuracy: 97.1%\n",
            "Totali sotto 95% : 70/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quarta verifica con esito\n",
        "#  7.2 % sotto al 95 %\n",
        "# 92.8 % sopra al 95 %\n",
        "\n",
        "g1,g2 = 10,10\n",
        "batch_size = 256\n",
        "n_test = 20000\n",
        "gen = polar_generator(n_test,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "accs = []\n",
        "mm = 0\n",
        "tot = 1000\n",
        "for x in range(tot):\n",
        "  (theta,rho),y = next(gen)\n",
        "  score, _, acc = model.evaluate((theta,rho), y, batch_size=batch_size,verbose=0)\n",
        "  if acc < 0.95:\n",
        "    mm += 1\n",
        "    print(\"Sotto 95% : {}/{}\".format(mm, x+1))\n",
        "  accs.append(acc)\n",
        "\n",
        "\n",
        "acc = np.mean(accs)\n",
        "print('Accuracy: {:.1f}%'.format(acc*100))\n",
        "print('Totali sotto 95% : {}/{}'.format(mm,tot))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUDOiCvJDzd_",
        "outputId": "b348b656-f962-4f1a-93d3-47cfe38db062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sotto 95% : 1/32\n",
            "Sotto 95% : 2/63\n",
            "Sotto 95% : 3/67\n",
            "Sotto 95% : 4/80\n",
            "Sotto 95% : 5/82\n",
            "Sotto 95% : 6/120\n",
            "Sotto 95% : 7/121\n",
            "Sotto 95% : 8/162\n",
            "Sotto 95% : 9/184\n",
            "Sotto 95% : 10/192\n",
            "Sotto 95% : 11/198\n",
            "Sotto 95% : 12/207\n",
            "Sotto 95% : 13/235\n",
            "Sotto 95% : 14/250\n",
            "Sotto 95% : 15/295\n",
            "Sotto 95% : 16/316\n",
            "Sotto 95% : 17/320\n",
            "Sotto 95% : 18/327\n",
            "Sotto 95% : 19/331\n",
            "Sotto 95% : 20/340\n",
            "Sotto 95% : 21/344\n",
            "Sotto 95% : 22/358\n",
            "Sotto 95% : 23/380\n",
            "Sotto 95% : 24/384\n",
            "Sotto 95% : 25/388\n",
            "Sotto 95% : 26/392\n",
            "Sotto 95% : 27/461\n",
            "Sotto 95% : 28/479\n",
            "Sotto 95% : 29/490\n",
            "Sotto 95% : 30/510\n",
            "Sotto 95% : 31/512\n",
            "Sotto 95% : 32/523\n",
            "Sotto 95% : 33/527\n",
            "Sotto 95% : 34/531\n",
            "Sotto 95% : 35/544\n",
            "Sotto 95% : 36/556\n",
            "Sotto 95% : 37/560\n",
            "Sotto 95% : 38/569\n",
            "Sotto 95% : 39/584\n",
            "Sotto 95% : 40/587\n",
            "Sotto 95% : 41/591\n",
            "Sotto 95% : 42/604\n",
            "Sotto 95% : 43/621\n",
            "Sotto 95% : 44/629\n",
            "Sotto 95% : 45/635\n",
            "Sotto 95% : 46/638\n",
            "Sotto 95% : 47/665\n",
            "Sotto 95% : 48/713\n",
            "Sotto 95% : 49/714\n",
            "Sotto 95% : 50/716\n",
            "Sotto 95% : 51/725\n",
            "Sotto 95% : 52/727\n",
            "Sotto 95% : 53/729\n",
            "Sotto 95% : 54/746\n",
            "Sotto 95% : 55/757\n",
            "Sotto 95% : 56/783\n",
            "Sotto 95% : 57/785\n",
            "Sotto 95% : 58/794\n",
            "Sotto 95% : 59/803\n",
            "Sotto 95% : 60/808\n",
            "Sotto 95% : 61/831\n",
            "Sotto 95% : 62/858\n",
            "Sotto 95% : 63/861\n",
            "Sotto 95% : 64/875\n",
            "Sotto 95% : 65/891\n",
            "Sotto 95% : 66/914\n",
            "Sotto 95% : 67/917\n",
            "Sotto 95% : 68/921\n",
            "Sotto 95% : 69/925\n",
            "Sotto 95% : 70/940\n",
            "Sotto 95% : 71/957\n",
            "Sotto 95% : 72/961\n",
            "Accuracy: 97.0%\n",
            "Totali sotto 95% : 72/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quinta verifica con esito del 6.1 %\n",
        "#  6.1 % sotto al 95 %\n",
        "# 93.9 % sopra al 95 %\n",
        "\n",
        "g1,g2 = 10,10\n",
        "batch_size = 256\n",
        "n_test = 20000\n",
        "gen = polar_generator(n_test,grid=(g1,g2),noise=0.002,flat=True)\n",
        "\n",
        "accs = []\n",
        "mm = 0\n",
        "tot = 1000\n",
        "for x in range(tot):\n",
        "  (theta,rho),y = next(gen)\n",
        "  score, _, acc = model.evaluate((theta,rho), y, batch_size=batch_size,verbose=0)\n",
        "  if acc < 0.95:\n",
        "    mm += 1\n",
        "    print(\"Sotto 95% : {}/{}\".format(mm, x+1))\n",
        "  accs.append(acc)\n",
        "\n",
        "\n",
        "acc = np.mean(accs)\n",
        "print('Accuracy: {:.1f}%'.format(acc*100))\n",
        "print('Totali sotto 95% : {}/{}'.format(mm,tot))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDHizKcNGij_",
        "outputId": "31ee43a3-6542-474f-c863-54cb47485387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sotto 95% : 1/9\n",
            "Sotto 95% : 2/21\n",
            "Sotto 95% : 3/33\n",
            "Sotto 95% : 4/58\n",
            "Sotto 95% : 5/96\n",
            "Sotto 95% : 6/111\n",
            "Sotto 95% : 7/136\n",
            "Sotto 95% : 8/155\n",
            "Sotto 95% : 9/182\n",
            "Sotto 95% : 10/188\n",
            "Sotto 95% : 11/190\n",
            "Sotto 95% : 12/193\n",
            "Sotto 95% : 13/200\n",
            "Sotto 95% : 14/208\n",
            "Sotto 95% : 15/251\n",
            "Sotto 95% : 16/311\n",
            "Sotto 95% : 17/329\n",
            "Sotto 95% : 18/343\n",
            "Sotto 95% : 19/344\n",
            "Sotto 95% : 20/371\n",
            "Sotto 95% : 21/379\n",
            "Sotto 95% : 22/386\n",
            "Sotto 95% : 23/393\n",
            "Sotto 95% : 24/394\n",
            "Sotto 95% : 25/404\n",
            "Sotto 95% : 26/428\n",
            "Sotto 95% : 27/445\n",
            "Sotto 95% : 28/472\n",
            "Sotto 95% : 29/502\n",
            "Sotto 95% : 30/533\n",
            "Sotto 95% : 31/551\n",
            "Sotto 95% : 32/552\n",
            "Sotto 95% : 33/554\n",
            "Sotto 95% : 34/564\n",
            "Sotto 95% : 35/600\n",
            "Sotto 95% : 36/644\n",
            "Sotto 95% : 37/671\n",
            "Sotto 95% : 38/702\n",
            "Sotto 95% : 39/719\n",
            "Sotto 95% : 40/721\n",
            "Sotto 95% : 41/728\n",
            "Sotto 95% : 42/735\n",
            "Sotto 95% : 43/741\n",
            "Sotto 95% : 44/753\n",
            "Sotto 95% : 45/782\n",
            "Sotto 95% : 46/788\n",
            "Sotto 95% : 47/792\n",
            "Sotto 95% : 48/805\n",
            "Sotto 95% : 49/811\n",
            "Sotto 95% : 50/876\n",
            "Sotto 95% : 51/912\n",
            "Sotto 95% : 52/919\n",
            "Sotto 95% : 53/929\n",
            "Sotto 95% : 54/943\n",
            "Sotto 95% : 55/945\n",
            "Sotto 95% : 56/946\n",
            "Sotto 95% : 57/957\n",
            "Sotto 95% : 58/969\n",
            "Sotto 95% : 59/986\n",
            "Sotto 95% : 60/993\n",
            "Sotto 95% : 61/998\n",
            "Accuracy: 97.0%\n",
            "Totali sotto 95% : 61/1000\n"
          ]
        }
      ]
    }
  ]
}